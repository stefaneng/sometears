---
title: "sometears"
subtitle: "Continuous structure learning in R"
author: ["Jacob Vidergar",
  "Ye Tian", 
  "Stefan Eng"]
date: "`r Sys.Date()`"
engine: knitr
bibliography: sometears.bib
execute:
  echo: false
  message: false
  warning: false
format:
  pdf:
    fig-pos: 'H'
    documentclass: article
    fontsize: 12pt
    geometry:
      - margin=1in
    template-partials:
      - tex_partials/title.tex
      - tex_partials/before-body.tex
include-in-header:
  text: |
    \usepackage{setspace}
    \onehalfspacing
    \linespread{1.5}
abstract: |
  Abstract
logo: ../../sometears_logo.png
github: https://github.com/stefaneng/885-Project
---

\setlength{\abovedisplayskip}{1pt}
\setlength{\belowdisplayskip}{1pt}

```{r setup}
#| echo: false
#| message: false
#| warning: false
library(sometears)
library(knitr)
library(ggplot2)
library(dplyr)
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
colortheme2 <- c("#FDB44E", "skyblue")
theme_set(theme_classic())
```


## Introduction

We implemented an R package that reproduces the TOPO (linear) algorithm [@deng2023] as well as the DAGMA (linear) algorithm [@bello2023] in R.
These algorithm provides an innovative solution to the non-convex optimization problems associated with learning the structure of Directed Acyclic Graphs (DAGs).
The project is named after the original continuous acyclicity function NOTEARS [@zheng2018].

## Background

## Algorithm

## Evaluation

### Linear SEM Simulation

We evaluate the performance of the algorithms on a simulated linear SEM dataset.
For this we simulate similarly to [@deng2023] by drawing random DAGs and then simulating data from them.
We evaluated by sampling an $d = 8$ dimensional adjacency matrix $B$, with sparsity of 80% with edge weights drawn from $[-2, -0.5] \cup [0.5, 2]$ uniformly.
We simulated noise as independent normal $\epsilon \sim N(0, \sigma^2 = 0.25)$ such that the data is multivariate normal $X \sim N(0, (I - W)^{-1} \text{diag}(0.5) (I - W)^{-T})$.

The three algorithm that we tested are:

1. DAGMA using torch (using ADAM and L-BFGS-B)
2. DAGMA using `lfgbs` R package to handle L1 optimization
3. TOPO

We noted that DAGMA is quite sensitive to the choice of hyperparameters.
We use the loss function multiplier $\mu = (10, 5, 1, 0.1)$, the log-determinant parameter $s = 1.1$, and $\beta_1 = 0.1$ (L1 penalty).
We did implement DAGMA using the torch ADAM automatic differentiation method in R but found that without careful selection of hyper-parameters the optimization often failed.
Using the L-BFGS-B optimizer within torch performed better and was less sensitive to the choice of hyper-parameters.
The DAGMA implementation in python uses a custom ADAM optimizer with some better logic for re-adjusting hyper-parameters during the optimization if the solution diverges from a valid solution.
This was not easy to do with torch in R.

```{r results-plot}
#| label: fig-results-plot
#| fig-height: 6
#| fig-width: 8
#| fig-cap: "Comparison of the two DAGMA methods and the TOPO method on a linear SEM simulation."

eval_results_10 <- readRDS("evaluation_results_10.rds")
eval_results_15 <- readRDS("evaluation_results_15.rds")

results_to_df <- function(results) {
  bind_rows(lapply(results, function(x) {
    bind_rows(
      tibble::rownames_to_column(
        data.frame(
          value = c(x$confusion_torch$overall, x$confusion_torch$byClass)
          ), "metric") %>%
        tibble::add_column(method = "DAGMA-torch"),
      tibble::rownames_to_column(
        data.frame(
          value = c(x$confusion_lbfgs$overall, x$confusion_lbfgs$byClass)
          ), "metric") %>%
        tibble::add_column(method = "DAGMA-lfgbs"),
      tibble::rownames_to_column(
        data.frame(
          value = c(x$confusion_topo$overall, x$confusion_topo$byClass)
          ), "metric") %>%
        tibble::add_column(method = "TOPO")
    )
  }), .id = "id")
}

confusion_results <- bind_rows(
  cbind(results_to_df(eval_results_10), d = 10),
  cbind(results_to_df(eval_results_15), d = 15)
)

confusion_results %>%
  filter(metric %in% c("Accuracy", "Sensitivity", "Specificity", "F1")) %>%
ggplot(., aes(x = method, y = value, fill = as.factor(d))) +
  facet_wrap(~metric, scales = "free") +
  geom_boxplot() +
  labs(x = "Method",
       y = "Accuracy") +
  scale_fill_manual(values = colortheme2, name = "Dimension")
```

## Contributions

Jacob and Ye independently implemented an initial version of the TOPO algorithm which we combined into a single version as well as running the python version on the Sachs dataset.
Stefan implemented the DAGMA algorithms, the linear SEM evaluation and wrote the R package, the documentation and miscellaneous functions.

\newpage

## References

::: {#refs}
:::
