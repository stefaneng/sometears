---
title: "sometears"
subtitle: "Continuous structure learning in R"
author: ["Jacob Vidergar",
  "Ye Tian", 
  "Stefan Eng"]
date: "`r Sys.Date()`"
engine: knitr
bibliography: sometears.bib
execute:
  echo: false
  message: false
  warning: false
format:
  pdf:
    fig-pos: 'H'
    documentclass: article
    fontsize: 12pt
    geometry:
      - margin=1in
    template-partials:
      - tex_partials/title.tex
      - tex_partials/before-body.tex
include-in-header:
  text: |
    \usepackage{setspace}
    \usepackage{algorithm}
    \usepackage{algpseudocode}
    \onehalfspacing
    \linespread{1.5}
abstract: |
  Abstract
logo: ../../sometears_logo.png
github: https://github.com/stefaneng/885-Project
---

\setlength{\abovedisplayskip}{1pt}
\setlength{\belowdisplayskip}{1pt}

```{r setup}
#| echo: false
#| message: false
#| warning: false
library(sometears)
library(knitr)
library(ggplot2)
library(dplyr)
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
colortheme2 <- c("#FDB44E", "skyblue")
theme_set(theme_classic())
```


## Introduction

We implemented an R package that reproduces the TOPO (linear) algorithm [@deng2023] as well as the DAGMA (linear) algorithm [@bello2023] in R.
These algorithm provides an innovative solution to the non-convex optimization problems associated with learning the structure of Directed Acyclic Graphs (DAGs).
The project is named after the original continuous acyclicity function NOTEARS [@zheng2018].

## Background

## Algorithm

### DAGMA

The DAGMA algorithm [@bello2023] is a continuous optimization algorithm that learns the structure of a Directed Acyclic Graph (DAG) from data.
The goal is to optimize the objective function 

$$
\min_{\theta} Q(\theta; X) + \beta_1 \| \theta\|_1 \quad \text{subject to } h_{ldet}^s(W(\theta)) = 0
$$
where $Q$ is the loss function, $s$ is a regularization parameter, and $\beta_1$ is a $\ell_1$ regularization parameter.
$W$ is the adjacency matrix of the DAG represented by the parameters $\theta$ which we will refer to as $W$, dropping the $\theta$ and assume that $W$ simply creates a matrix from the parameter vector $\theta$.
The continuous acyclicity function $h_{\text{ldet}}(W)$ is used to enforce the acyclicity constraint and is defined as:

$$
\begin{aligned}
h_{\text{ldet}}^s(W) &:= -\log \det (s I - W \circ W) + d \log s\\
\nabla h_{\text{ldet}}^s(W) &:= 2(sI - W \circ W)^{-T} \circ W
\end{aligned}
$$

The authors note that _each_ computation of $h_{ldet}$ takes $O(d^3)$ operations.

The algorithm starts by optimizing more heavily towards the loss function and $\ell_1$ norm and gradually shifts towards making the solution acyclic.

- Input:
  - $X$: Data matrix
  - $s$: Regularization parameter (larger than the spectral radius of $X$)
  - $\mu$: Weights on the loss function e.g. $\mu = c(10, 5, 1, 0.1)$ 
  - $\beta_1$: $\ell_1$ regularization parameter (e.g. $0.1$)
- Output: $W$, the adjacency matrix of the DAG that best maximizes the objective function.

For each $\mu_t \in mu$ solve the optimization problem:

$$
\theta^{(t + 1)} = \underset{\theta}{\text{arg min}}~ \mu_t \left[ Q(\theta; X) +  \beta_1 \| \theta\|_1 \right] + h_{ldet}^s(W(\theta))
$$

The DAGMA algorithm does not specify a specific way to solve this problem but the authors use the ADAM optimizer [@kingma2017].

### TOPO
The TOPO algorithm [@deng2023] aims to solve the same problem as DAGMA but aims to do so by iteratively swapping elements in a valid topological order so that the solutions is always acyclic.

## Evaluation

### Linear SEM Simulation

We evaluate the performance of the algorithms on a simulated linear SEM dataset.
For this we simulate similarly to [@deng2023] by drawing random DAGs and then simulating data from them.
We evaluated by sampling an $d = 8$ dimensional adjacency matrix $B$, with sparsity of 80% with edge weights drawn from $[-2, -0.5] \cup [0.5, 2]$ uniformly.
We simulated noise as independent normal $\epsilon \sim N(0, \sigma^2 = 0.25)$ such that the data is multivariate normal $X \sim N(0, (I - W)^{-1} \text{diag}(0.5) (I - W)^{-T})$.

The three algorithm that we tested are:

1. DAGMA using torch (using ADAM and L-BFGS-B)
2. DAGMA using `lfgbs` R package to handle L1 optimization
3. TOPO

We noted that DAGMA is quite sensitive to the choice of hyperparameters.
We use the loss function multiplier $\mu = (10, 5, 1, 0.1)$, the log-determinant parameter $s = 1.1$, and $\beta_1 = 0.1$ (L1 penalty).
We did implement DAGMA using the torch ADAM automatic differentiation method in R but found that without careful selection of hyper-parameters the optimization often failed.
Using the L-BFGS-B optimizer within torch performed better and was less sensitive to the choice of hyper-parameters.
The DAGMA implementation in python uses a custom ADAM optimizer with some better logic for re-adjusting hyper-parameters during the optimization if the solution diverges from a valid solution.
We decided to focus our efforts on the TOPO implementation rather than optimizing the DAGMA implementation.

```{r results-plot}
#| label: fig-results-plot
#| fig-height: 6
#| fig-width: 8
#| fig-cap: "Comparison of the two DAGMA methods and the TOPO method on a linear SEM simulation."

conusion_results <- readRDS("confusion_results_plot.rds")

confusion_results %>%
  filter(metric %in% c("Accuracy", "Sensitivity", "Specificity", "F1")) %>%
ggplot(., aes(x = method, y = value, fill = as.factor(d))) +
  facet_wrap(~metric, scales = "free") +
  geom_boxplot() +
  labs(x = "Method",
       y = "Accuracy") +
  scale_fill_manual(values = colortheme2, name = "Dimension")
```

### Sachs Dataset

The Sachs dataset [@sachs2005] is a commonly used dataset benchmark in the study of causal network discovery.
This dataset contains data on protein-signaling networks of continuous measurements of protein and phospholipid expression levels in human immune cells obtained through single cell analysis.
The reason this dataset is used is that there is a known gold standard causal network that we can compare our results to.


## Contributions

Jacob and Ye independently implemented an initial version of the TOPO algorithm which we combined into a single version as well as running the python version on the Sachs dataset.
Stefan implemented the DAGMA algorithms, the linear SEM evaluation and wrote the R package, the documentation and miscellaneous functions.

\newpage

## References

::: {#refs}
:::
